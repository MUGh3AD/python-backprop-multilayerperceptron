{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e18975-daa3-4cdd-8bf8-2077fbc3b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe0e874e-32a4-4592-8485-9b017af65304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value class is essentially scalars with additional features enabling back propigation\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = data\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self._backprop = lambda: None\n",
    "        self._prev = _children\n",
    "        self.grad = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value object (Data: {self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "        def _backprop():\n",
    "            local_det = 1 # in respect to both terms, d(out)/d(term) = 1\n",
    "            back_prop_det = out.grad * local_det # Apply the chain rule\n",
    "            self.grad += back_prop_det # Pass gradient \"backwards\" to the inputs\n",
    "            other.grad += back_prop_det # Pass gradient \"backwards\" to the inputs\n",
    "            \n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out =  Value(self.data * other.data, (self, other), \"*\")\n",
    "        def _backprop():\n",
    "            self_local_det = other.data\n",
    "            other_local_det = self.data\n",
    "            self.grad += self_local_det * out.grad\n",
    "            other.grad += other_local_det * out.grad\n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __trudiv__(self, other):\n",
    "        return self * other**-1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def tanh(self):\n",
    "        tanh_value = (math.exp(2*self.data)-1)/(math.exp(2*self.data) + 1)\n",
    "        out = Value(tanh_value, (self, ), \"tanh\")\n",
    "        def _backprop():\n",
    "            self.grad += (1 - (tanh_value**2)) * out.grad\n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,  ), 'exp')\n",
    "\n",
    "        def _backprop(self):\n",
    "            self.grad += out.data * out.grad\n",
    "        \n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        assert isinstance(power, (int, float)), \"currently pow only supports int/float\"\n",
    "        x = self.data\n",
    "        out = Value(x**power, (self, ), \"**\")\n",
    "\n",
    "        def _backprop():\n",
    "            self.grad += (power*(x**(power-1))) * out.grad\n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def backprop(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1\n",
    "        for node in reversed(topo):\n",
    "            node._backprop()\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa8f55e0-46b4-4d5f-9764-42dbf6abf343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Value objects to be used \n",
    "x1 = Value(2)\n",
    "x2 = Value(0)\n",
    "w1 = Value(-3)\n",
    "w2 = Value(1)\n",
    "b = Value(6.8813735870195432)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bec8119-9cea-4648-92eb-47ebc64bd850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do/dx1 : -1.4999999999999996\n",
      "Equation output value : 0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "# Example scalar equation. Using backprop to see effect of x1 on o (gradient do/dx1)\n",
    "n = (x1*w1 + x2*w2) + b\n",
    "o = n.tanh()\n",
    "o.backprop()\n",
    "print(\"do/dx1 : %s\" % (x1.grad))\n",
    "print(\"Equation output value : %s\" % o.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ebf04-0bc2-43fa-a2c5-4313aa63f00f",
   "metadata": {},
   "source": [
    "### PyTorch Version of above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e2f3bd-d784-445f-8874-146ca6491b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2437d246-fe41-4783-8621-d03b826852d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do/dx1 : -1.5000003851533106\n",
      "Equation output value : 0.7071066904050358\n"
     ]
    }
   ],
   "source": [
    "# Same as above, just using PyTorch api rather than custom Value class\n",
    "x1 = torch.Tensor([2]).double(); x1.requires_grad = True\n",
    "x2 = torch.Tensor([0]).double(); x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3]).double(); w1.requires_grad = True\n",
    "w2 = torch.Tensor([1]).double(); w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double(); b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "o.backward()\n",
    "print(\"do/dx1 : %s\" % (x1.grad.item()))\n",
    "print(\"Equation output value : %s\" % o.data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01871001-59b3-4b7d-8931-a90451ffbbb0",
   "metadata": {},
   "source": [
    "### Value class based Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bce4ea6-f598-4255-8328-f7d43928fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron Class builds on Value class to create a neuron with a set of (randomly initiated) weights and a bias\n",
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        '''\n",
    "        nin - number of inputs to the neuron\n",
    "        '''\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)] # Generate a random weight for each input\n",
    "        self.b = Value(random.uniform(-1,1)) # Generate a random bias for the neuron\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # [weights DOT inputs] plus bias\n",
    "        paired_weights = zip(self.w, x) # Merge into [(w1, x1), (w2, x2)... ]\n",
    "        dot_prod = 0\n",
    "        for pair in paired_weights:\n",
    "            dot_prod = pair[0]*pair[1] + dot_prod\n",
    "        activation = dot_prod + self.b\n",
    "        # Oneliner: activation = sum(w*x for w,x in zip(self.w, x), self.b)  # sum(<array to sum>, <starting value to add on top of>)\n",
    "\n",
    "        # Non liniarity, using tanh here\n",
    "        out = activation.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        # Get the 'knobs and dials' for this neuron\n",
    "        return self.w + [self.b] # Full list of modifiable params\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086f8046-9a1c-4630-8a61-f3d4062274b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (3, 6)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Zip Demo - used to \"pair up\" values in arrays\n",
    "zip_x = [1,2,3]\n",
    "zip_y = [4,5,6]\n",
    "list(zip(zip_x,zip_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ad3db13-d57f-458d-bcd2-0b7ed5ba461e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value object (Data: -0.903335923815253)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0,3.0]\n",
    "n = Neuron(2)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe66dd4-a56f-4356-bb32-8aee2cc26474",
   "metadata": {},
   "source": [
    "### From Neurons to Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad89d1b-d33f-482a-949b-62e59ea4e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer class brings together a set of neurons into a network layer\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, input):\n",
    "        # To run an input on a layer, run the input through each neuron\n",
    "        return [n(input) for n in self.neurons]\n",
    "\n",
    "    def parameters(self):\n",
    "        # Get the 'knobs and dials' for this layer\n",
    "        params = []\n",
    "        for n in self.neurons:\n",
    "            params.extend(n.parameters())\n",
    "\n",
    "        # shorthand: return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e13c7454-b97a-4a5c-8e2b-fcc73ad5e980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value object (Data: -0.9514688550081641),\n",
       " Value object (Data: 0.8824649501675387),\n",
       " Value object (Data: -0.948712358937843)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create inputs and a layer of neurons\n",
    "inputs=[1.0, -2.0, -1.0, 4]\n",
    "l1 = Layer(4, 3)\n",
    "l1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131ac57-3f4c-49f9-846d-4fabffb3cc17",
   "metadata": {},
   "source": [
    "#### Full MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0c6a19f-1178-48c4-a982-26f5a517cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP class implements a multilayer perceptron model as a collection of connected layers\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        '''\n",
    "        nouts -> list of output sizes desired\n",
    "        '''\n",
    "        layer_sizes = [nin] + nouts # Add inputs to the start of the array of sizes\n",
    "        self.layers = [Layer(layer_sizes[n], layer_sizes[n+1]) for n in range(len(nouts))] # Create layers\n",
    "\n",
    "    def __call__(self, input):\n",
    "        '''\n",
    "        Run an input through the whole network.\n",
    "        '''\n",
    "        x = input\n",
    "        for  l in self.layers:\n",
    "            x = l(x) # Feed output from previous layer into next layer\n",
    "        return x[0] if len(x) == 1 else x # Clean output for last node\n",
    "\n",
    "    def parameters(self):\n",
    "        # Get the 'knobs and dials' for the whole network\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5a9372f-0df3-47d5-b41f-bc6f0b59d763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value object (Data: 0.4048014958084447)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=[1.0, -2.0, -1.0]\n",
    "mlp = MLP(3, [4,4,1]) # Create 3 layer network that takes 3 inputs\n",
    "mlp_result = mlp(inputs) \n",
    "mlp_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4693a4-a1cf-4ccb-989d-8c554f5fefb8",
   "metadata": {},
   "source": [
    "### Back Prop in the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5599442-3dab-4f71-b4c3-06f757d0f8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value object (Data: 0.9069534342036834),\n",
       " Value object (Data: 0.9163948866459092),\n",
       " Value object (Data: 0.9332324724623532),\n",
       " Value object (Data: 0.775155971557185)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results with no training, just passing through a new MLP\n",
    "input_data = [\n",
    "    [1.0, 3.0, -1],\n",
    "    [1.0, 3.0, 0.5],\n",
    "    [0.5, 1.5, 1.0],\n",
    "    [-2.0, 1.0, -1.0]\n",
    "]\n",
    "targets = [1.0, -1.0, -1.0, 1.0]  # example 1 -> 1.0; example 2 -> -1\n",
    "\n",
    "untrained_results = [mlp(i) for i in input_data]\n",
    "untrained_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05c289e1-d80d-4db5-adba-bca88ef2a046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value object (Data: 7.469169654678371)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating loss for no training\n",
    "loss = [(result - target)**2 for target, result in zip(targets, untrained_results)]\n",
    "loss = sum(loss, Value(0))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52203b61-4b62-46f6-9e96-f9c1937c9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec87d9a6-8c94-48af-a828-356337ced46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.036800686038540545"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.layers[0].neurons[0].w[0].grad # See the gradient of loss func in respect to one weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee8a0e1b-1853-40ca-9032-abd130f697fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlp.parameters()) # Number of params in this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09337e29-2fa9-4916-a99a-ab45e9e95114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value object (Data: 0.00027476512146774347)\n"
     ]
    }
   ],
   "source": [
    "# Training loop on MLP,  \n",
    "i=0\n",
    "input_data = [\n",
    "    [1.0, 3.0, -1],\n",
    "    [1.0, 3.0, 0.5],\n",
    "    [0.5, 1.5, 1.0],\n",
    "    [-2.0, 1.0, -1.0]\n",
    "]\n",
    "loss = Value(0)\n",
    "for _ in range(1000): # 1000 training itters\n",
    "    net_outs = [mlp(i) for i in input_data]\n",
    "    loss = sum(((result - target)**2 for target, result in zip(targets, net_outs)), Value(0))\n",
    "    loss.backprop()\n",
    "    for p in mlp.parameters(): # Move value in direction of gradient descent\n",
    "        step_size = 0.5 # Small step size to avoid over stepping\n",
    "        p.data += -p.grad * step_size # Step in direction of descent\n",
    "        p.grad = 0 # Reset each parameters gradient to 0 to stop backprop from adding to last run's gradient\n",
    "\n",
    "print(loss) # New loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76a0dd90-9716-4364-b818-799d5ce75238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9906714455268741, -0.990225840923678, -0.9923404647776453, 0.9942195374714783]\n",
      "[1.0, -1.0, -1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Visually compare targets to outputs after training\n",
    "print([mlp(v).data for v in input_data])\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f2209-502e-421b-8f31-9110d6a9a110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
